\section{The Methodological Approach}

\subsection{Handcrafted Features}
We started with a traditional approach to handcrafted features [GIUSEPPE SPIEGA].

\subsection{Convolutional Neural Network}
According to the state of the art, the approach that now seems to perform better for these tasks is the one based on MFCC and Convolutional Neural Network. 

We started with the architecture suggested by "Daniel Kostrzewa" \cite{kostrzewa2021music}.
"Layer 1 and 2 have both 64 kernels each, whereas layers 3 and 4 have 128 kernels. The kernel size of all layers is equal to 5. After each layer, there is 2-D max pooling applied with kernel size and stride equal 2. In every convolutional layer, ReLU is used as an activation function. Batch normalization is performed afterward. The convolutional layers are followed by one fully connected linear layer with linear activation function and the final output of 8 nodes" \cite{kostrzewa2021music}. Dropout probability is 0.20 .

\begin{figure}[ht]
\centering
\includegraphics[scale=0.6]{images/CNN-architecture.png}
\caption{The architecture suggested by "Daniel Kostrzewa" \cite{kostrzewa2021music}.}
\label{fig:GP_Acq}
\end{figure}

We then tried to perform a hyper-parameter optimization o this architecture.
Not being able to do an optimization from start to finish and on all the hyper-parameters space, we decided to use HyperBand and to optimize only some hyper-parameters.

The hyper-parameters considered for the optimization were: 
\begin{itemize}
    \item The size of the kernel (fixed on each layer) in [3,6].
    \item The number of kernels for each layer in [32, 256].
    \item The probability of dropout in \{0.2, 0.25, 0.5\}.
    \item The initial learning rate in \{0.01, 0.001, 0.0001\}.
\end{itemize}

The optimization of the network has led to have, in the case without date augmentation, the first layer has 32 kernels, the second one has 128, the third 192 and the fourth has 98. The kernel size of all layers is equal to 6. Dropout probability is 0.25 and initial learning rate is 0.001.

While in the case with data augmentation we have, the first layer has X kernels, the second one has X, the third X and the fourth has X. The kernel size of all layers is equal to X. Dropout probability is X and initial learning rate is X.

\subsection{Feature extraction with Convolutional Neural Network}


